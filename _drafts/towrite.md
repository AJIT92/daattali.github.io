http://rpubs.com/daattali/heatmapsGgplotVsLattice

all my extensions

google forms with shiny (use S3 or [mondodb](https://www.mongolab.com) or [mysql](http://www.freemysqlhosting.net/)  http://shiny.rstudio.com/articles/share-data.html 
mysql: con <- DBI::dbConnect(RMySQL::MySQL(), group = "deang", default.file=normalizePath("~/.my.cnf"))
not concerned about safety/input sanitization
doesnt seem like RMySQL supports preparesd statemetns so need to "hack" together sql statement creation with paste/sprintf
install sqlite: `sudo apt-get install sqlite3 libsqlite3-dev`, create database: `sqlite3 databasename.db`


(lots of cutting corners: not considering any other social media, not looking at # of times post was shared directly from r-bloggers website, not looking at how much discussion the post generated by only at number of favorites and retweets)
didnt do statistical significance tests, didnt include fb data, didnt include # of shares via tweet button because it looks like most posts pre 2014 mostly have 0 so it'll introduce an unfair bias towards new posts

```
library(httr)
library(XML)
library(plyr)
library(dplyr)
library(magrittr)
library(twitteR)
library(ggplot2)

# for wordcloud
library(SnowballC)
library(wordcloud)
library(tm)

#setup_twitter_oauth(...)

# Grab last 3200 tweets (restriction set by Twitter)
MAX_TWEETS <- 3200
tweets_raw <- userTimeline('Rbloggers', n = MAX_TWEETS,
                           includeRts = FALSE, excludeReplies = TRUE)

# Prepare a nice data.frame with the info we want
# I chose to keep the last URL in each tweet because that's the URL
# that points to the r-bloggers post
tweets <- 
  ldply(tweets_raw, function(x) {
    data_frame(id = x$id,
               date = as.Date(x$created),
               day = weekdays(date),
               favorites = x$favoriteCount,
               retweets = x$retweetCount,
               title = x$text,
               url = x$urls %>% .[['url']] %>% tail(1)
    )
  })
  
# Remove the URL and #rstats hashtag from the tweet title
tweets$title <- mapply(gsub, sprintf(" %s #rstats", tweets$url), "", tweets$title)

# Older posts all contain "This article was originally posted ... and kindly contributed..." 
tweets$title <- gsub(": \n\n\\(This article.*$", "", tweets$title)

# Add author name to each post

get_post_author_single <- function(url) {
  if (is.null(url)) {
    return(NA)
  }
  
  # get author HTML node
  author_node <- 
    GET(url) %>%
    content("parsed") %>%
    getNodeSet("//a[@rel='author']")
  if (author_node %>% length != 1) {
    return(NA)
  }

  # get author name
  author <- author_node %>% .[[1]] %>% xmlValue
  
  # r-bloggers hides email address namesm so grab the name a different way
  if (grepl("\\[email\\sprotected\\]", author)) {
    author <- author_node %>% .[[1]] %>% xmlGetAttr("title")
  }
  
  author  
}

get_post_author <- function(urls) {
  lapply(urls, get_post_author_single) %>% unlist
}

# this will take several minutes because we're scraping r-bloggers
tweets %<>% mutate(author = get_post_author(url))  

# remove NA author (these are mostly jobs posts, plus a few blog posts that have been deleted)
tweets %<>% na.omit

# merge tweets that have a duplicate title+author signature
# (there are some tweets with the exact same title and author that are a day apart
# for some reason, and they have different URLs because they do appear on r-bloggers
# twice, so here I'm trying to merge these cases. It's not the most efficient way
# to do it, but it's very readable so I'll stick with it.)
tweets %<>%
  ddply(.(author, title), function(x) {
    x$favorites <- sum(x$favorites)
    x$retweets <- sum(x$retweets)
    x[1, , drop = FALSE]
  })

# calculate a tweet score metric
# this is very arbitrary - I chose to score a tweet's success in this
# way because it's simple and there are roughly twice as many retweets
# as favorites in total.
tweets$score <- tweets$favorites + tweets$retweets * 2
sum(tweets$favorites) / sum(tweets$retweets)  # = 2.11


tweets %<>% factor(levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

nrow(tweets)  # we're left with 2970 tweets after cleanup


# write.csv(tweets, "tweets.csv", quote = TRUE, row.names = FALSE)



# preliminary look at data
ggplot(tweets) +
  geom_point(aes(favorites, retweets), size = 3, shape = 21, fill = "#888888") +
  theme_classic(30)


tweets_by_author <- ddply(tweets, ~ author, function(x) {
    data.frame(num = nrow(x),
               favorites = sum(x$favorites),
               retweets = sum(x$retweets),
               avg_score = mean(x$score) %>% round(digits = 1)
    )}) %>%
    arrange(desc(avg_score)) 
# I'm in the top 10 of avg score!
nrow(tweets_by_author)  # 420 unique authors since Sept 2013, so about 1/4 of the authors on r-bloggers haven't posted since I started grad school until today
ggplot(tweets_by_author) + geom_histogram(aes(num), binwidth = 1, fill = "#888888", color = "#444444") + theme_classic(30) + scale_x_continuous(limits = c(1, 50), breaks = c(1, seq(10, 50, 10))) + xlab("# of posts") + ylab("# of blogs who have\nexactly x posts") + ggtitle("How much does each blog\ncontribute?") + coord_flip()
# looks like a lot of people only posted once since... 

# who are the top contributors?
tweets_by_author %>% arrange(desc(num)) %>% .[1:10, , drop = FALSE]

# I expect to see a very strong correlation between # of times a tweet
# is favourited vs retweeted
cor(tweets_by_author$favorites, tweets_by_author$retweets)
# indeed, very strong correlation

# I wonder if users who post more also tend to post higher quality content?
cor(tweets_by_author$num, tweets_by_author$avg_score)
# looks like a pretty convincing 0 correlation there.

# Now let's see who the best scorers are, which blogs consistently
# contribute posts that get shared a lot
tweets_by_author %>% arrange(desc(avg_score)) %>% .[1:10, , drop = FALSE]
# First impression: **my name is there!** Woo! :D 
# Looks like a lot of one-hit wonders, which at first seems contradicting to the previous result because it looked like there is no relationship between # of posts and average post score. But this result **does** make sense because of two reasons: first of all, about 1/3 of the authors only have one article, so all other things equal, we expect 1/3 of the top posts to be by them. Secondly, it's much easier to put a lot of effort to produce one great and useful post rather than doing it over and over again.

# Let's see what all the posts looks like for the top scorers
top_authors <- tweets_by_author %>% arrange(desc(avg_score)) %>% .$author %>% .[1:10]
ggplot(tweets %>% filter(author %in% top_authors)) +
  geom_point(aes(favorites, retweets, fill = author), size = 5, shape = 21) +
  theme_classic(30) +
  scale_fill_brewer("Author", type = "qual", palette = 3)

# And let's see them again, in perspective to all tweets
ggplot(tweets) +
  geom_point(aes(favorites, retweets), size = 3, shape = 21, fill = "#888888") +
  theme_classic(30) +
  geom_point(data = tweets %>% filter(author %in% top_authors),
             aes(favorites, retweets, fill = author), size = 5, shape = 21) +
  scale_fill_brewer("Author", type = "qual", palette = 3)





tweets_by_day <-
  ddply(tweets, ~ day, function(x) {
    data.frame(num = nrow(x),
               favorites_per_post = sum(x$favorites) / nrow(x),
               retweets_per_post = sum(x$retweets) / nrow(x),
               avg_score = mean(x$score)
    )})
# cool! looks like weekend (sat/sunday) get the least posts, BUT they both have the highest number of favorites and retweets!

ggplot(tweets, aes(x = day, y = score, cex = 1.7), shape = 21) +
    geom_jitter(aes( fill = day), position = position_jitter(height = 0.4), show_guide=FALSE,shape=21, color = "#333333") + geom_line(data=tweets_by_day,aes(day,avg_score, group=1),color="#333333",size=1,)+ geom_point(data=tweets_by_day,aes(day, avg_score), shape =20,size=10,col="#333333") + theme_bw()


```
# wordcloud of most popular words in r-bloggers post titles
topwords <- 
    tweets$title %>%
    paste(collapse = " ") %>%
    stringr::str_split("\\s") %>%
    unlist %>%
    tolower %>%
    removePunctuation %>%
    removeWords(stopwords("english")) %>%
    wordStem %>%
    .[. != ""] %>% 
    .[. != "r"] %>%
    table %>%
    sort(decreasing = TRUE) %>%
    .[1:100]

wordcloud(names(topwords), topwords, min.freq = 10)
